{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Jf8zy4X3b3Q"
      },
      "outputs": [],
      "source": [
        "# 1. Google Drive'ı Colab'a Bağlama\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--sNkhQX3q-G",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install https://huggingface.co/turkish-nlp-suite/tr_core_news_trf/resolve/main/tr_core_news_trf-1.0-py3-none-any.whl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqLhtpKcXOmj"
      },
      "source": [
        "#**TOPİC MODELLİNG**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_jpKYuZuj8W"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re\n",
        "from datetime import datetime\n",
        "import os\n",
        "import spacy\n",
        "from spacy.lang.tr.stop_words import STOP_WORDS\n",
        "import multiprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUj6zzQh5SYb"
      },
      "outputs": [],
      "source": [
        "multiprocessing.set_start_method(\"spawn\", force=True)\n",
        "spacy.require_gpu()\n",
        "print(\"GPU kullanılıyor mu?\", spacy.prefer_gpu())\n",
        "\n",
        "nlp = spacy.load(\"tr_core_news_trf\")\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1tNh6Kl5eej"
      },
      "outputs": [],
      "source": [
        "def load_data(file_path, sheet_name=0):\n",
        "    return pd.read_excel(file_path, sheet_name=sheet_name)\n",
        "\n",
        "data_path = 'data/tweet_dataset.xlsx'\n",
        "\n",
        "if not os.path.exists(data_path):\n",
        "    from google.colab import files\n",
        "    print(\"Lütfen Excel dosyanızı yükleyin.\")\n",
        "    uploaded = files.upload()\n",
        "    for filename in uploaded.keys():\n",
        "        os.rename(filename, data_path)\n",
        "else:\n",
        "    print(\"Veri dosyası mevcut.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAV12VxO5n-j"
      },
      "outputs": [],
      "source": [
        "df = load_data(data_path)\n",
        "\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df.set_index('date', inplace=True)\n",
        "filtered_df = df[(df.index.year > 2021) & (df.index.year <= 2024)]\n",
        "monthly_groups = filtered_df.groupby(pd.Grouper(freq='M'))\n",
        "\n",
        "print(\"Veri hazırlandı ve aylık gruplandı.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjnz_aOF3r5W"
      },
      "outputs": [],
      "source": [
        "def preprocess_parallel(texts, batch_size=100, n_process=4):\n",
        "    processed = []\n",
        "    for doc in nlp.pipe(texts, batch_size=batch_size, n_process=n_process):\n",
        "        tokens = [\n",
        "            token.lemma_ for token in doc\n",
        "            if token.is_alpha and token.lemma_ not in stopwords and len(token.lemma_) > 1\n",
        "        ]\n",
        "        processed.append(tokens)\n",
        "    return processed\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_topic_modeling(processed_texts, num_topics=5, passes=15, iterations=400, chunksize=2000):\n",
        "    dictionary = corpora.Dictionary(processed_texts)\n",
        "    corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
        "\n",
        "    if len(dictionary) == 0:\n",
        "        return None, None, None\n",
        "\n",
        "    lda_model = LdaModel(\n",
        "        corpus=corpus,\n",
        "        id2word=dictionary,\n",
        "        num_topics=num_topics,\n",
        "        random_state=100,\n",
        "        update_every=1,\n",
        "        chunksize=chunksize,\n",
        "        passes=passes,\n",
        "        iterations=iterations,\n",
        "        alpha='auto',\n",
        "        per_word_topics=True,\n",
        "    )\n",
        "    return lda_model, corpus, dictionary\n"
      ],
      "metadata": {
        "id": "PiR7Alkn45MS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NThxDB-hVtMa"
      },
      "source": [
        "#**TOPİC MODELLİNG GÖRSELLEŞTİRME**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUe6Epw6hNih"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "from itertools import combinations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXM4VvkcImuZ"
      },
      "outputs": [],
      "source": [
        "def extract_topics(model, prefix, num_topics=100, num_words=10):\n",
        "    \"\"\"\n",
        "    Her bir topic için kelime–ağırlık sözlüğü döner.\n",
        "    Topic ID'lerine prefix ekleyerek benzersizleştirir.\n",
        "    \"\"\"\n",
        "    topics = model.print_topics(num_topics=num_topics, num_words=num_words)\n",
        "    result = {}\n",
        "    for tid, text in topics:\n",
        "        uid = f\"{prefix}_T{tid}\"\n",
        "        weights = {\n",
        "            w.strip().split(\"*\")[1].replace('\"',''): float(w.strip().split(\"*\")[0])\n",
        "            for w in text.split(\"+\")\n",
        "        }\n",
        "        result[uid] = weights\n",
        "    return result\n",
        "\n",
        "def cosine_sim(d1, d2):\n",
        "    \"\"\"İki ağırlık sözlüğü arasındaki kosinüs benzerliğini hesaplar.\"\"\"\n",
        "    all_w = set(d1) | set(d2)\n",
        "    v1 = np.array([d1.get(w,0) for w in all_w])\n",
        "    v2 = np.array([d2.get(w,0) for w in all_w])\n",
        "    dot = np.dot(v1, v2)\n",
        "    norm = np.linalg.norm(v1)*np.linalg.norm(v2)\n",
        "    return dot/norm if norm else 0.0\n",
        "\n",
        "def build_graph(topics, threshold=0.3):\n",
        "    \"\"\"\n",
        "    topics: {topic_id: {word:weight}}\n",
        "    Benzerlik > threshold olan topic’ler arasında kenar oluşturur.\n",
        "    \"\"\"\n",
        "    G = nx.Graph()\n",
        "    G.add_nodes_from(topics.keys())\n",
        "    for t1, t2 in combinations(topics, 2):\n",
        "        sim = cosine_sim(topics[t1], topics[t2])\n",
        "        if sim > threshold:\n",
        "            G.add_edge(t1, t2, weight=sim)\n",
        "    return G\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models_dir = \"models/lda\"\n",
        "for fname in sorted(os.listdir(models_dir)):\n",
        "    if not fname.endswith(\".model\"):\n",
        "        continue\n",
        "    period = fname.replace(\"lda_\", \"\").replace(\".model\", \"\")\n",
        "    mdl = LdaModel.load(os.path.join(models_dir, fname))\n",
        "    tp = extract_topics(mdl, prefix=period)\n",
        "    G = build_graph(tp, threshold=0.3)\n",
        "\n",
        "    # Baskın topic\n",
        "    cent = nx.degree_centrality(G)\n",
        "    dom = max(cent, key=cent.get)\n",
        "    print(f\"{period}: Dominant Topic = {dom} (score={cent[dom]:.3f})\")\n",
        "\n",
        "    # Görselleştir\n",
        "    plt.figure(figsize=(8,6))\n",
        "    pos = nx.spring_layout(G, seed=42, k=0.5)\n",
        "    widths = [2*d['weight'] for _,_,d in G.edges(data=True)]\n",
        "    nx.draw(G, pos, with_labels=True, node_size=300, width=widths)\n",
        "    plt.title(f\"{period} Topic Graph\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "eZj9w9Tk6IMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tüm periodlerden topic’leri topla\n",
        "all_topics = {}\n",
        "for fname in sorted(os.listdir(models_dir)):\n",
        "    if fname.endswith(\".model\"):\n",
        "        p = fname.replace(\"lda_\", \"\").replace(\".model\", \"\")\n",
        "        m = LdaModel.load(os.path.join(models_dir, fname))\n",
        "        all_topics.update(extract_topics(m, prefix=p))\n",
        "\n",
        "# Global ağ\n",
        "G_global = build_graph(all_topics, threshold=0.3)\n",
        "\n",
        "# En baskın iki topic\n",
        "cent_glob = nx.degree_centrality(G_global)\n",
        "top2 = sorted(cent_glob.items(), key=lambda x: x[1], reverse=True)[:2]\n",
        "\n",
        "print(\"== Global Dominant Topics ==\")\n",
        "for tid, score in top2:\n",
        "    print(f\"- {tid}: score={score:.3f}\")\n",
        "\n",
        "# Global grafiği çiz\n",
        "plt.figure(figsize=(10,8))\n",
        "pos = nx.spring_layout(G_global, seed=42, k=0.3)\n",
        "nx.draw(G_global, pos, node_size=50, with_labels=False, width=0.5)\n",
        "plt.title(\"Global Topic Network\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-Glvdtn86KFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = 'outputs/lda_models'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for month, group in monthly_groups:\n",
        "    month_str = month.strftime('%Y_%m')\n",
        "    print(f\"\\n--- {month_str} ---\")\n",
        "\n",
        "    texts = group['tweet'].dropna().tolist()\n",
        "    if not texts:\n",
        "        print(\"Metin bulunamadı.\")\n",
        "        continue\n",
        "\n",
        "    processed = preprocess_parallel(texts)\n",
        "    lda_model, corpus, dictionary = perform_topic_modeling(processed, num_topics=100)\n",
        "\n",
        "    if lda_model is None:\n",
        "        print(\"Yetersiz veri.\")\n",
        "        continue\n",
        "\n",
        "    print(\"Konular:\")\n",
        "    for idx, topic in lda_model.print_topics(num_topics=100, num_words=10):\n",
        "        print(f\"Konu #{idx+1}: {topic}\")\n",
        "\n",
        "    lda_model.save(os.path.join(output_dir, f'lda_{month_str}.model'))\n",
        "    dictionary.save(os.path.join(output_dir, f'dict_{month_str}.dict'))\n",
        "    corpora.MmCorpus.serialize(os.path.join(output_dir, f'corpus_{month_str}.mm'), corpus)\n"
      ],
      "metadata": {
        "id": "D5r2eF5a46zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdR6Ia5C2SJ3"
      },
      "source": [
        "#**EN ÇOK KULLANILAN KELİMELER VE ONLARLA BERABER EN ÇOK KULLANILAN KELİMELER**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PCXXfBCMp5Bp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from spacy.lang.de.stop_words import STOP_WORDS\n",
        "from collections import Counter\n",
        "\n",
        "# Almanca spaCy modeli\n",
        "nlp = spacy.load(\"de_core_news_sm\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2awGqZG7eaL2"
      },
      "outputs": [],
      "source": [
        "keyword_groups = {\n",
        "    \"turk\":    [\"türkisch\", \"türkei\"],\n",
        "    \"migration\": [\"abwanderung\", \"auswanderung\", \"migration\", \"immigrant\", \"einwanderer\"],\n",
        "    \"syrien\":  [\"syrien\", \"syrisch\", \"syrer\"]\n",
        "}\n",
        "\n",
        "# Her grup için birlikte geçen kelimeleri sayacak Counter\n",
        "group_counters = {grp: Counter() for grp in keyword_groups}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keyword_groups = {\n",
        "    \"turk\":    [\"türkisch\", \"türkei\"],\n",
        "    \"migration\": [\"abwanderung\", \"auswanderung\", \"migration\", \"immigrant\", \"einwanderer\"],\n",
        "    \"syrien\":  [\"syrien\", \"syrisch\", \"syrer\"]\n",
        "}\n",
        "\n",
        "# Her grup için birlikte geçen kelimeleri sayacak Counter\n",
        "group_counters = {grp: Counter() for grp in keyword_groups}\n"
      ],
      "metadata": {
        "id": "CT4OI6PtXEjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_file = \"outputs/keyword_context.xlsx\"\n",
        "with pd.ExcelWriter(output_file) as writer:\n",
        "    for grp, counter in group_counters.items():\n",
        "        top25 = counter.most_common(25)\n",
        "        pd.DataFrame(top25, columns=['context_word', 'frequency']) \\\n",
        "          .to_excel(writer, sheet_name=grp, index=False)\n",
        "\n",
        "print(f\"Kaydedildi: {output_file}\")\n"
      ],
      "metadata": {
        "id": "Dp61VbNBB67L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrGLRMGyEZgW"
      },
      "source": [
        "# **TR HATESPEECH EMOTİON**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "w-pkWspcvmk5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "import locale\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import torch\n",
        "from transformers import pipeline, logging\n",
        "\n",
        "# Ortam ve hata ayarları\n",
        "os.environ[\"HF_HUB_DOWNLOAD_TIMEOUT\"] = \"60\"\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "# GPU kullanılabiliyorsa zorunlu kıl\n",
        "if torch.cuda.is_available():\n",
        "    spacy.require_gpu()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1DMuMevEfR-"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"tr_core_news_trf\")\n",
        "nlp.disable_pipes(\"ner\", \"parser\")  # Gereksiz bileşenleri kapat\n",
        "\n",
        "data_path = \"data/turkish_tweets.xlsx\"\n",
        "df = pd.read_excel(data_path)\n",
        "\n",
        "# Tweetleri lemmatize et\n",
        "preprocessed_texts = [\n",
        "    \" \".join([token.lemma_ for token in doc if token.is_alpha])\n",
        "    for doc in nlp.pipe(df['tweet'].astype(str), batch_size=32, n_process=1)\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_pipeline = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"maymuni/bert-base-turkish-cased-emotion-analysis\",\n",
        "    tokenizer=\"maymuni/bert-base-turkish-cased-emotion-analysis\",\n",
        "    truncation=True,\n",
        "    framework=\"pt\",\n",
        "    device=0,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "hate_pipeline = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"barandinho/distilbert-base-turkish-cased-toxic-lang\",\n",
        "    tokenizer=\"barandinho/distilbert-base-turkish-cased-toxic-lang\",\n",
        "    truncation=True,\n",
        "    framework=\"pt\",\n",
        "    device=0,\n",
        "    batch_size=32\n",
        ")\n"
      ],
      "metadata": {
        "id": "DF7QuTi9Uv4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_results = emotion_pipeline(preprocessed_texts, batch_size=32)\n",
        "hate_results = hate_pipeline(preprocessed_texts, batch_size=32)\n",
        "\n",
        "df[\"emotion_label\"] = [r[\"label\"] for r in emotion_results]\n",
        "df[\"emotion_score\"] = [r[\"score\"] for r in emotion_results]\n",
        "df[\"hate_speech_label\"] = [r[\"label\"] for r in hate_results]\n",
        "df[\"hate_speech_score\"] = [r[\"score\"] for r in hate_results]\n"
      ],
      "metadata": {
        "id": "5fVyYjBH7Tlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = \"outputs/tr_emotion_hate_results.xlsx\"\n",
        "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "\n",
        "df.to_excel(output_path, index=False)\n",
        "print(f\"Sonuçlar kaydedildi: {output_path}\")\n"
      ],
      "metadata": {
        "id": "4Dv8nQI_7Uwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz3CCObMW8iD"
      },
      "source": [
        "# **DE HATESPEECH EMOTİON**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MD_WfB9WvRSr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from transformers import pipeline\n",
        "\n",
        "# SpaCy Almanca modelini yükle\n",
        "nlp = spacy.load(\"de_core_news_sm\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEYcrhkDUR8A"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    doc = nlp(text)\n",
        "    return \" \".join([token.lemma_ for token in doc])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLyVWiwUm-IT"
      },
      "outputs": [],
      "source": [
        "hate_pipeline = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"chrisrtt/gbert-multi-class-german-hate\",\n",
        "    tokenizer=\"chrisrtt/gbert-multi-class-german-hate\",\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "def predict_hate_speech(text):\n",
        "    try:\n",
        "        processed = preprocess_text(text)\n",
        "        result = hate_pipeline(processed)[0]\n",
        "        return result[\"label\"], result[\"score\"]\n",
        "    except:\n",
        "        return \"error\", None\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_pipeline = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"visegradmedia-emotion/Emotion_RoBERTa_german6_v7\",\n",
        "    tokenizer=\"visegradmedia-emotion/Emotion_RoBERTa_german6_v7\",\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "def predict_emotion(text):\n",
        "    try:\n",
        "        processed = preprocess_text(text)\n",
        "        result = emotion_pipeline(processed)[0]\n",
        "        return result[\"label\"], result[\"score\"]\n",
        "    except:\n",
        "        return \"error\", None\n"
      ],
      "metadata": {
        "id": "Mqf3OGvD__gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"data/german_tweets.xlsx\"\n",
        "df = pd.read_excel(data_path)\n",
        "\n",
        "def classify_row(text):\n",
        "    hate_label, hate_score = predict_hate_speech(text)\n",
        "    emo_label, emo_score = predict_emotion(text)\n",
        "    return pd.Series([hate_label, hate_score, emo_label, emo_score])\n",
        "\n",
        "df[['hate_speech_label', 'hate_speech_score', 'emotion_label', 'emotion_score']] = df['tweet'].apply(classify_row)\n"
      ],
      "metadata": {
        "id": "YI6IYNp-ABB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_mapping = {\n",
        "    \"LABEL_0\": \"anger\",\n",
        "    \"LABEL_1\": \"fear\",\n",
        "    \"LABEL_2\": \"disgust\",\n",
        "    \"LABEL_3\": \"sadness\",\n",
        "    \"LABEL_4\": \"joy\",\n",
        "    \"LABEL_5\": \"none\"\n",
        "}\n",
        "\n",
        "df[\"emotion_label\"] = df[\"emotion_label\"].map(label_mapping)\n"
      ],
      "metadata": {
        "id": "h2xJbjzLADJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = \"outputs/de_emotion_hate_results.xlsx\"\n",
        "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "\n",
        "df.to_excel(output_path, index=False)\n",
        "print(f\"Sonuçlar kaydedildi: {output_path}\")\n"
      ],
      "metadata": {
        "id": "J1nTNgNcAEVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJD8XgLphV40"
      },
      "source": [
        "# **KELİME FREKANSLARI**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0EHbj1jjMlT",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from collections import Counter\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "spacy.prefer_gpu()\n",
        "\n",
        "# Türkçe transformer tabanlı modelin yüklenmesi\n",
        "nlp = spacy.load(\"tr_core_news_trf\")\n",
        "\n",
        "# Stopword listesi\n",
        "from spacy.lang.tr.stop_words import STOP_WORDS\n",
        "stopwords = list(STOP_WORDS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSPk-tUlhah7"
      },
      "outputs": [],
      "source": [
        "df = pd.read_excel(\"data/turkish_tweets.xlsx\")\n",
        "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "df['year_month'] = df['date'].dt.strftime('%Y-%m')\n",
        "\n",
        "tweets = df['tweet'].astype(str).tolist()\n",
        "year_months = df['year_month'].tolist()\n",
        "\n",
        "monthly_tokens = {ym: [] for ym in set(year_months)}\n",
        "\n",
        "for ym, doc in zip(year_months, nlp.pipe(tweets, batch_size=50)):\n",
        "    tokens = [token.lemma_.lower() for token in doc if token.is_alpha and token.lemma_.lower() not in stopwords]\n",
        "    monthly_tokens[ym].extend(tokens)\n",
        "\n",
        "monthly_top50 = {\n",
        "    ym: Counter(tokens).most_common(50) for ym, tokens in monthly_tokens.items()\n",
        "}\n",
        "\n",
        "output_path = \"outputs/tr_top50_by_month.xlsx\"\n",
        "os.makedirs(os\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keyword_groups = {\n",
        "    \"suriye\": [\"suriye\", \"suriyeli\"],\n",
        "    \"goc\": [\"göç\", \"göçmen\", \"göçmenler\"],\n",
        "    \"alman\": [\"alman\", \"almanlar\", \"almanya\"]\n",
        "}\n",
        "\n",
        "group_counters = {group: Counter() for group in keyword_groups}\n",
        "\n",
        "df = pd.read_excel(\"data/turkish_tweets.xlsx\")\n",
        "tweets = df[\"tweet\"].astype(str).tolist()\n",
        "\n",
        "for doc in nlp.pipe(tweets, batch_size=50, n_process=2):\n",
        "    tokens = list(doc)\n",
        "    for i, token in enumerate(tokens):\n",
        "        token_text = token.text.lower()\n",
        "        for group, keywords in keyword_groups.items():\n"
      ],
      "metadata": {
        "id": "L2occcGfT7oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = \"outputs/tr_keyword_context.xlsx\"\n",
        "with pd.ExcelWriter(output_path) as writer:\n",
        "    for group, counter in group_counters.items():\n",
        "        df_out = pd.DataFrame(counter.most_common(25), columns=[\"Kelime\", \"Frekans\"])\n",
        "        df_out.to_excel(writer, sheet_name=group, index=False)\n",
        "\n",
        "print(f\"Anahtar kelime bağlam analizleri kaydedildi: {output_path}\")\n"
      ],
      "metadata": {
        "id": "GV3vGvEkXSYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(\"outputs/tr_emotion_hate_results.xlsx\")\n",
        "df.loc[df['emotion_label'] == 'surpriz', 'emotion_label'] = 'şaşkın'\n",
        "df.to_excel(\"outputs/tr_emotion_hate_results_updated.xlsx\", index=False)\n",
        "print(\"Etiket düzeltmeleri yapıldı ve dosya güncellendi.\")\n"
      ],
      "metadata": {
        "id": "VPrX2EYHCh-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ZAMAN SERİSİ ANALİZİ**"
      ],
      "metadata": {
        "id": "6ZwNV-9s3jBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Dosya yolu\n",
        "input_path = \"outputs/Hate_Emotion_Analysis_Final.xlsx\"\n",
        "df = pd.read_excel(input_path)\n",
        "\n",
        "# Tarih sütunu dönüştürme ve filtreleme\n",
        "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "df = df[df['date'] >= '2023-06-01']\n",
        "df['year_month'] = df['date'].dt.to_period('M').astype(str)\n"
      ],
      "metadata": {
        "id": "VV-xD78c3inm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hate Speech yüzdeleri\n",
        "hate_groups = (\n",
        "    df.groupby('year_month')['hate_speech_label']\n",
        "    .value_counts(normalize=True)\n",
        "    .rename('percentage')\n",
        "    .mul(100)\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Emotion yüzdeleri\n",
        "emotion_groups = (\n",
        "    df.groupby('year_month')['emotion_label']\n",
        "    .value_counts(normalize=True)\n",
        "    .rename('percentage')\n",
        "    .mul(100)\n",
        "    .reset_index()\n",
        ")\n"
      ],
      "metadata": {
        "id": "Yzh2WPEvCrSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"plots/hate_speech\", exist_ok=True)\n",
        "os.makedirs(\"plots/emotion\", exist_ok=True)\n"
      ],
      "metadata": {
        "id": "PO4SObhZCsvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for cat in hate_groups['hate_speech_label'].unique():\n",
        "    data = hate_groups[hate_groups['hate_speech_label'] == cat].copy()\n",
        "    data.sort_values('year_month', inplace=True)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(data['year_month'], data['percentage'], marker='o')\n",
        "    plt.title(f\"Hate Speech Oranı: '{cat}'\")\n",
        "    plt.xlabel(\"Yıl-Ay\")\n",
        "    plt.ylabel(\"Yüzde (%)\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.savefig(f\"plots/hate_speech/hate_{cat}.png\")\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "VQRo_5MkCtrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for cat in emotion_groups['emotion_label'].unique():\n",
        "    data = emotion_groups[emotion_groups['emotion_label'] == cat].copy()\n",
        "    data.sort_values('year_month', inplace=True)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(data['year_month'], data['percentage'], marker='o')\n",
        "    plt.title(f\"Duygu Oranı: '{cat}'\")\n",
        "    plt.xlabel(\"Yıl-Ay\")\n",
        "    plt.ylabel(\"Yüzde (%)\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.savefig(f\"plots/emotion/emotion_{cat}.png\")\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "gXgNNqceCvyp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}